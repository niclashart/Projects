@misc{amiriChunkTwiceEmbed2025,
  title = {Chunk {{Twice}}, {{Embed Once}}: {{A Systematic Study}} of {{Segmentation}} and {{Representation Trade-offs}} in {{Chemistry-Aware Retrieval-Augmented Generation}}},
  shorttitle = {Chunk {{Twice}}, {{Embed Once}}},
  author = {Amiri, Mahmoud and Bocklitz, Thomas},
  year = 2025,
  month = jun,
  number = {arXiv:2506.17277},
  eprint = {2506.17277},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.17277},
  urldate = {2025-10-29},
  abstract = {Retrieval-Augmented Generation (RAG) systems are increasingly vital for navigating the ever-expanding body of scientific literature, particularly in high-stakes domains such as chemistry. Despite the promise of RAG, foundational design choices -- such as how documents are segmented and represented -- remain underexplored in domain-specific contexts. This study presents the first large-scale, systematic evaluation of chunking strategies and embedding models tailored to chemistry-focused RAG systems. We investigate 25 chunking configurations across five method families and evaluate 48 embedding models on three chemistry-specific benchmarks, including the newly introduced QuestChemRetrieval dataset. Our results reveal that recursive token-based chunking (specifically R100-0) consistently outperforms other approaches, offering strong performance with minimal resource overhead. We also find that retrieval-optimized embeddings -- such as Nomic and Intfloat E5 variants -- substantially outperform domain-specialized models like SciBERT. By releasing our datasets, evaluation framework, and empirical benchmarks, we provide actionable guidelines for building effective and efficient chemistry-aware RAG systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Physics - Chemical Physics},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/JHGWNAVY/Amiri und Bocklitz - 2025 - Chunk Twice, Embed Once A Systematic Study of Segmentation and Representation Trade-offs in Chemist.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/7V523NK9/2506.html}
}

@misc{AttentionAllYou,
  title = {Attention {{Is All You Need}}},
  journal = {ar5iv},
  urldate = {2025-11-02},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through a\dots},
  howpublished = {https://ar5iv.labs.arxiv.org/html/1706.03762},
  langid = {english},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/WDVAIW8C/1706.html}
}

@misc{balaguerRAGVsFinetuning2024,
  title = {{{RAG}} vs {{Fine-tuning}}: {{Pipelines}}, {{Tradeoffs}}, and a {{Case Study}} on {{Agriculture}}},
  shorttitle = {{{RAG}} vs {{Fine-tuning}}},
  author = {Balaguer, Angels and Benara, Vinamra and Cunha, Renato Luiz de Freitas and Filho, Roberto de M. Estev{\~a}o and Hendry, Todd and Holstein, Daniel and Marsman, Jennifer and Mecklenburg, Nick and Malvar, Sara and Nunes, Leonardo O. and Padilha, Rafael and Sharp, Morris and Silva, Bruno and Sharma, Swati and Aski, Vijay and Chandra, Ranveer},
  year = 2024,
  month = jan,
  number = {arXiv:2401.08406},
  eprint = {2401.08406},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.08406},
  urldate = {2025-10-15},
  abstract = {There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47\% to 72\%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/YY8L37KZ/Balaguer et al. - 2024 - RAG vs Fine-tuning Pipelines, Tradeoffs, and a Case Study on Agriculture.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/I9JJJ2B4/2401.html}
}

@misc{bhatRethinkingChunkSize2025,
  title = {Rethinking {{Chunk Size For Long-Document Retrieval}}: {{A Multi-Dataset Analysis}}},
  shorttitle = {Rethinking {{Chunk Size For Long-Document Retrieval}}},
  author = {Bhat, Sinchana Ramakanth and Rudat, Max and Spiekermann, Jannis and {Flores-Herr}, Nicolas},
  year = 2025,
  month = may,
  number = {arXiv:2505.21700},
  eprint = {2505.21700},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.21700},
  urldate = {2025-10-29},
  abstract = {Chunking is a crucial preprocessing step in retrieval-augmented generation (RAG) systems, significantly impacting retrieval effectiveness across diverse datasets. In this study, we systematically evaluate fixed-size chunking strategies and their influence on retrieval performance using multiple embedding models. Our experiments, conducted on both short-form and long-form datasets, reveal that chunk size plays a critical role in retrieval effectiveness -- smaller chunks (64-128 tokens) are optimal for datasets with concise, fact-based answers, whereas larger chunks (512-1024 tokens) improve retrieval in datasets requiring broader contextual understanding. We also analyze the impact of chunking on different embedding models, finding that they exhibit distinct chunking sensitivities. While models like Stella benefit from larger chunks, leveraging global context for long-range retrieval, Snowflake performs better with smaller chunks, excelling at fine-grained, entity-based matching. Our results underscore the trade-offs between chunk size, embedding models, and dataset characteristics, emphasizing the need for improved chunk quality measures, and more comprehensive datasets to advance chunk-based retrieval in long-document Information Retrieval (IR).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/K4CRRPZY/Bhat et al. - 2025 - Rethinking Chunk Size For Long-Document Retrieval A Multi-Dataset Analysis.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/X3IQTU9K/2505.html}
}

@misc{davidjonietzBuildingEvaluatingMultilingual2024,
  title = {{Building and evaluating multilingual RAG systems}},
  author = {Davidjonietz},
  year = 2024,
  month = nov,
  journal = {Data Science at Microsoft},
  urldate = {2025-11-01},
  abstract = {By David Jonietz and Marco Zatta},
  langid = {ngerman},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/LBXYXHQL/building-and-evaluating-multilingual-rag-systems-943c290ab711.html}
}

@misc{dongSCANSemanticDocument2025,
  title = {{{SCAN}}: {{Semantic Document Layout Analysis}} for {{Textual}} and {{Visual Retrieval-Augmented Generation}}},
  shorttitle = {{{SCAN}}},
  author = {Dong, Yuyang and Ueda, Nobuhiro and Boros, Kriszti{\'a}n and Ito, Daiki and Sera, Takuya and Oyamada, Masafumi},
  year = 2025,
  month = may,
  number = {arXiv:2505.14381},
  eprint = {2505.14381},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.14381},
  urldate = {2025-09-13},
  abstract = {With the increasing adoption of Large Language Models (LLMs) and Vision-Language Models (VLMs), rich document analysis technologies for applications like Retrieval-Augmented Generation (RAG) and visual RAG are gaining significant attention. Recent research indicates that using VLMs can achieve better RAG performance, but processing rich documents still remains a challenge since a single page contains large amounts of information. In this paper, we present SCAN (\textbackslash textbf\textbraceleft S\textbraceright emanti\textbackslash textbf\textbraceleft C\textbraceright{} Document Layout \textbackslash textbf\textbraceleft AN\textbraceright alysis), a novel approach enhancing both textual and visual Retrieval-Augmented Generation (RAG) systems working with visually rich documents. It is a VLM-friendly approach that identifies document components with appropriate semantic granularity, balancing context preservation with processing efficiency. SCAN uses a coarse-grained semantic approach that divides documents into coherent regions covering continuous components. We trained the SCAN model by fine-tuning object detection models with sophisticated annotation datasets. Our experimental results across English and Japanese datasets demonstrate that applying SCAN improves end-to-end textual RAG performance by up to 9.0\textbackslash\% and visual RAG performance by up to 6.4\textbackslash\%, outperforming conventional approaches and even commercial document processing solutions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/Y5PXLTRU/Dong et al. - 2025 - SCAN Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/T6K32AD2/2505.html}
}

@misc{gaoRetrievalAugmentedGenerationLarge2024,
  title = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}},
  author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
  year = 2024,
  month = mar,
  number = {arXiv:2312.10997},
  eprint = {2312.10997},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10997},
  urldate = {2025-09-13},
  abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/U83NLSQL/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language Models A Survey.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/7JQPNFGM/2312.html}
}

@misc{guoRAGAnythingAllinOneRAG2025,
  title = {{{RAG-Anything}}: {{All-in-One RAG Framework}}},
  shorttitle = {{{RAG-Anything}}},
  author = {Guo, Zirui and Ren, Xubin and Xu, Lingrui and Zhang, Jiahao and Huang, Chao},
  year = 2025,
  month = oct,
  number = {arXiv:2510.12323},
  eprint = {2510.12323},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.12323},
  urldate = {2025-10-15},
  abstract = {Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between current RAG capabilities and real-world information environments. Modern knowledge repositories are inherently multimodal, containing rich combinations of textual content, visual elements, structured tables, and mathematical expressions. Yet existing RAG frameworks are limited to textual content, creating fundamental gaps when processing multimodal documents. We present RAG-Anything, a unified framework that enables comprehensive knowledge retrieval across all modalities. Our approach reconceptualizes multimodal content as interconnected knowledge entities rather than isolated data types. The framework introduces dual-graph construction to capture both cross-modal relationships and textual semantics within a unified representation. We develop cross-modal hybrid retrieval that combines structural knowledge navigation with semantic matching. This enables effective reasoning over heterogeneous content where relevant evidence spans multiple modalities. RAG-Anything demonstrates superior performance on challenging multimodal benchmarks, achieving significant improvements over state-of-the-art methods. Performance gains become particularly pronounced on long documents where traditional approaches fail. Our framework establishes a new paradigm for multimodal knowledge access, eliminating the architectural fragmentation that constrains current systems. Our framework is open-sourced at: https://github.com/HKUDS/RAG-Anything.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/IINSFAEK/Guo et al. - 2025 - RAG-Anything All-in-One RAG Framework.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/T5YBYX29/2510.html}
}

@misc{guptaComprehensiveSurveyRetrievalAugmented2024,
  title = {A {{Comprehensive Survey}} of {{Retrieval-Augmented Generation}} ({{RAG}}): {{Evolution}}, {{Current Landscape}} and {{Future Directions}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Retrieval-Augmented Generation}} ({{RAG}})},
  author = {Gupta, Shailja and Ranjan, Rajesh and Singh, Surya Narayan},
  year = 2024,
  month = oct,
  number = {arXiv:2410.12837},
  eprint = {2410.12837},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.12837},
  urldate = {2025-10-15},
  abstract = {This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/IY2CSDPU/Gupta et al. - 2024 - A Comprehensive Survey of Retrieval-Augmented Generation (RAG) Evolution, Current Landscape and Fut.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/IYNT6LHL/2410.html}
}

@article{hambardeInformationRetrievalRecent2023,
  title = {Information {{Retrieval}}: {{Recent Advances}} and {{Beyond}}},
  shorttitle = {Information {{Retrieval}}},
  author = {Hambarde, Kailash A. and Proenca, Hugo},
  year = 2023,
  journal = {IEEE Access},
  volume = {11},
  eprint = {2301.08801},
  primaryclass = {cs},
  pages = {76581--76604},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3295776},
  urldate = {2025-10-25},
  abstract = {In this paper, we provide a detailed overview of the models used for information retrieval in the first and second stages of the typical processing chain. We discuss the current state-of-the-art models, including methods based on terms, semantic retrieval, and neural. Additionally, we delve into the key topics related to the learning process of these models. This way, this survey offers a comprehensive understanding of the field and is of interest for for researchers and practitioners entering/working in the information retrieval domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/MPHBPPRP/Hambarde und Proenca - 2023 - Information Retrieval Recent Advances and Beyond.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/NHNGH26N/2301.html}
}

@misc{khanDevelopingRetrievalAugmented2024,
  title = {Developing {{Retrieval Augmented Generation}} ({{RAG}}) Based {{LLM Systems}} from {{PDFs}}: {{An Experience Report}}},
  shorttitle = {Developing {{Retrieval Augmented Generation}} ({{RAG}}) Based {{LLM Systems}} from {{PDFs}}},
  author = {Khan, Ayman Asad and Hasan, Md Toufique and Kemell, Kai Kristian and Rasku, Jussi and Abrahamsson, Pekka},
  year = 2024,
  month = oct,
  number = {arXiv:2410.15944},
  eprint = {2410.15944},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.15944},
  urldate = {2025-11-02},
  abstract = {This paper presents an experience report on the development of Retrieval Augmented Generation (RAG) systems using PDF documents as the primary data source. The RAG architecture combines generative capabilities of Large Language Models (LLMs) with the precision of information retrieval. This approach has the potential to redefine how we interact with and augment both structured and unstructured knowledge in generative models to enhance transparency, accuracy, and contextuality of responses. The paper details the end-to-end pipeline, from data collection, preprocessing, to retrieval indexing and response generation, highlighting technical challenges and practical solutions. We aim to offer insights to researchers and practitioners developing similar systems using two distinct approaches: OpenAI's Assistant API with GPT Series and Llama's open-source models. The practical implications of this research lie in enhancing the reliability of generative AI systems in various sectors where domain-specific knowledge and real-time information retrieval is important. The Python code used in this work is also available at: https://github.com/GPT-Laboratory/RAG-LLM-Development-Guidebook-from-PDFs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Software Engineering},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/LTIMCSJL/Khan et al. - 2024 - Developing Retrieval Augmented Generation (RAG) based LLM Systems from PDFs An Experience Report.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/L498LAMW/2410.html}
}

@misc{laiEnhancingTechnicalDocuments2025,
  title = {Enhancing {{Technical Documents Retrieval}} for {{RAG}}},
  author = {Lai, Songjiang and Cheung, Tsun-Hin and Fung, Ka-Chun and Xue, Kaiwen and Lin, Kwan-Ho and Choi, Yan-Ming and Ng, Vincent and Lam, Kin-Man},
  year = 2025,
  month = sep,
  number = {arXiv:2509.04139},
  eprint = {2509.04139},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.04139},
  urldate = {2025-09-13},
  abstract = {In this paper, we introduce Technical-Embeddings, a novel framework designed to optimize semantic retrieval in technical documentation, with applications in both hardware and software development. Our approach addresses the challenges of understanding and retrieving complex technical content by leveraging the capabilities of Large Language Models (LLMs). First, we enhance user queries by generating expanded representations that better capture user intent and improve dataset diversity, thereby enriching the fine-tuning process for embedding models. Second, we apply summary extraction techniques to encode essential contextual information, refining the representation of technical documents. To further enhance retrieval performance, we fine-tune a bi-encoder BERT model using soft prompting, incorporating separate learning parameters for queries and document context to capture fine-grained semantic nuances. We evaluate our approach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that Technical-Embeddings significantly outperforms baseline models in both precision and recall. Our findings highlight the effectiveness of integrating query expansion and contextual summarization to enhance information access and comprehension in technical domains. This work advances the state of Retrieval-Augmented Generation (RAG) systems, offering new avenues for efficient and accurate technical document retrieval in engineering and product development workflows.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/G2MWMHCJ/Lai et al. - 2025 - Enhancing Technical Documents Retrieval for RAG.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/AIPG7EQ8/2509.html}
}

@article{lauMultimodalRAGAnalysis2024,
  title = {Multimodal {{RAG Analysis}} of {{Product Datasheet}}},
  author = {Lau, David and Samy, Ganthan Narayana and Rahim, Dr Fiza Abdul and Maarop, Nurazean and Selvananthan, Mahiswaran and Ali, Mazlan and Perumal, Sundresan},
  year = 2024,
  month = dec,
  journal = {Open International Journal of Informatics},
  volume = {12},
  number = {2},
  pages = {1--12},
  issn = {2289-2370},
  doi = {10.11113/oiji2024.12n2.309},
  urldate = {2025-09-13},
  abstract = {Large language models such as ChatGPT serves as multipurpose chatbot that can provide information across diverse disciplines. However, in order to generate timely and accurate response, retrieval-augmented generation method has been devised to enhance the response of these models. The release of vision models has paved the way for practitioners to perform multimodal retrieval augmented generation on documents that commonly consist of a combination of text, images and tables. Hence, this method is explored to analyze a product datasheet and match it with minimum specification required by potential clients. It is demonstrated that multimodal retrieval augmented generation performed better compared to basic retrieval augmented generation that did not consider the information contained in image and table specifically. While the performance of this method still lagged behind the commercially available GPT-4o, information is not exchanged with any external parties which could potentially address any privacy issue with regards to highly sensitive information. The incorporation of various best practices in this domain as highlighted in other studies can potentially improve the output generation of this method toward matching or exceeding the performance of commercially available tools.},
  copyright = {Copyright (c) 2024 Open International Journal of Informatics},
  langid = {english},
  keywords = {Multimodal RAG,Retrieval-augmented Generation,Vision Model},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/YJEGYNB8/Lau et al. - 2024 - Multimodal RAG Analysis of Product Datasheet.pdf}
}

@misc{lewisRetrievalAugmentedGenerationKnowledgeIntensive2021,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  year = 2021,
  month = apr,
  number = {arXiv:2005.11401},
  eprint = {2005.11401},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.11401},
  urldate = {2025-09-13},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/FN6X8HAP/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/V5JTZTXS/2005.html}
}

@article{mageshHallucinationFreeAssessingReliability2025,
  title = {Hallucination-{{Free}}? {{Assessing}} the {{Reliability}} of {{Leading}} {{{\textsc{AI}}}} {{Legal Research Tools}}},
  shorttitle = {Hallucination-{{Free}}?},
  author = {Magesh, Varun and Surani, Faiz and Dahl, Matthew and Suzgun, Mirac and Manning, Christopher D. and Ho, Daniel E.},
  year = 2025,
  month = jun,
  journal = {Journal of Empirical Legal Studies},
  volume = {22},
  number = {2},
  pages = {216--242},
  issn = {1740-1453, 1740-1461},
  doi = {10.1111/jels.12413},
  urldate = {2025-09-13},
  abstract = {ABSTRACT             Legal practice has witnessed a sharp rise in products incorporating artificial intelligence (AI). Such tools are designed to assist with a wide range of core legal tasks, from search and summarization of caselaw to document drafting. However, the large language models used in these tools are prone to ``hallucinate,'' or make up false information, making their use risky in high-stakes domains. Recently, certain legal research providers have touted methods such as retrieval-augmented generation (RAG) as ``eliminating'' or ``avoid[ing]'' hallucinations, or guaranteeing ``hallucination-free'' legal citations. Because of the closed nature of these systems, systematically assessing these claims is challenging. In this article, we design and report on the first preregistered empirical evaluation of AI-driven legal research tools. We demonstrate that the providers' claims are overstated. While hallucinations are reduced relative to general-purpose chatbots (GPT-4), we find that the AI research tools made by LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and Ask Practical Law AI) each hallucinate between 17\% and 33\% of the time. We also document substantial differences between systems in responsiveness and accuracy. Our article makes four key contributions. It is the first to assess and report the performance of RAG-based proprietary legal AI tools. Second, it introduces a comprehensive, preregistered dataset for identifying and understanding vulnerabilities in these systems. Third, it proposes a clear typology for differentiating between hallucinations and accurate legal responses. Last, it provides evidence to inform the responsibilities of legal professionals in supervising and verifying AI outputs, which remains a central open question for the responsible integration of AI into law.},
  langid = {english}
}

@article{manningIntroductionInformationRetrieval2009,
  title = {Introduction to {{Information Retrieval}}},
  author = {Manning, Christopher and Raghavan, Prabhakar and Schuetze, Hinrich},
  year = 2009,
  langid = {english},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/9RYN2S7B/Manning et al. - 2009 - Introduction to Information Retrieval.pdf}
}

@misc{meiSurveyMultimodalRetrievalAugmented2025,
  title = {A {{Survey}} of {{Multimodal Retrieval-Augmented Generation}}},
  author = {Mei, Lang and Mo, Siyu and Yang, Zhihan and Chen, Chong},
  year = 2025,
  month = mar,
  number = {arXiv:2504.08748},
  eprint = {2504.08748},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.08748},
  urldate = {2025-09-13},
  abstract = {Multimodal Retrieval-Augmented Generation (MRAG) enhances large language models (LLMs) by integrating multimodal data (text, images, videos) into retrieval and generation processes, overcoming the limitations of text-only Retrieval-Augmented Generation (RAG). While RAG improves response accuracy by incorporating external textual knowledge, MRAG extends this framework to include multimodal retrieval and generation, leveraging contextual information from diverse data types. This approach reduces hallucinations and enhances question-answering systems by grounding responses in factual, multimodal knowledge. Recent studies show MRAG outperforms traditional RAG, especially in scenarios requiring both visual and textual understanding. This survey reviews MRAG's essential components, datasets, evaluation methods, and limitations, providing insights into its construction and improvement. It also identifies challenges and future research directions, highlighting MRAG's potential to revolutionize multimodal information retrieval and generation. By offering a comprehensive perspective, this work encourages further exploration into this promising paradigm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Emerging Technologies,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/3LGWGXTL/Mei et al. - 2025 - A Survey of Multimodal Retrieval-Augmented Generation.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/DGZ95EG6/2504.html}
}

@article{murdacaKNOWLEDGEBASEDINFORMATIONEXTRACTION,
  title = {{{KNOWLEDGE-BASED INFORMATION EXTRACTION FROM DATASHEETS OF SPACE PARTS}}},
  author = {Murdaca, F and Berquand, A and Kumar, K and Riccardi, A and Soares, T and Geren{\'e}, S and Brauer, N},
  abstract = {Selection of the right space parts is an essential step during the design of complex engineering systems and requires information that is typically embedded in unstructured documents like datasheets, Interface Control Documents (ICDs) and technical manuals. Satsearch (https://satsearch.co) aims to consolidate global space supply chain information within a single platform, by converting unstructured datasheets into machine-readable, human-readable, electronic datasheets (EDS). After satsearch's initial, manual efforts at generating EDS from source datasheets for space parts, they realized that the process is not scalable. A possible solution is to employ knowledge base information extraction systems. The Design Engineering Assistant (DEA) team from the University of Strathclyde is currently working on the automation of the extraction of information from unstructured documents (e.g. textbooks, reports, datasheets, research papers, etc.) through the development of an expert system. This paper summarizes the approach and outcomes of a feasibility study for the DEA, assessing benefits and obstacles for the implementation of a fullyautomated information extraction process, focusing at this stage only on datasheets for space parts for preliminary mission design.},
  langid = {english},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/7KBLSSXM/Murdaca et al. - KNOWLEDGE-BASED INFORMATION EXTRACTION FROM DATASHEETS OF SPACE PARTS.pdf}
}

@misc{narayanDeepMMSearchR1EmpoweringMultimodal2025,
  title = {{{DeepMMSearch-R1}}: {{Empowering Multimodal LLMs}} in {{Multimodal Web Search}}},
  shorttitle = {{{DeepMMSearch-R1}}},
  author = {Narayan, Kartik and Xu, Yang and Cao, Tian and Nerella, Kavya and Patel, Vishal M. and Shiee, Navid and Grasch, Peter and Jia, Chao and Yang, Yinfei and Gan, Zhe},
  year = 2025,
  month = oct,
  number = {arXiv:2510.12801},
  eprint = {2510.12801},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.12801},
  urldate = {2025-10-15},
  abstract = {Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such as retrieval augmented generation (RAG) methods, search agents, and search equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and poorly constructed search queries, which result in inefficiencies and suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1, the first multimodal LLM capable of performing on-demand, multi-turn web searches and dynamically crafting queries for both image and text search tools. Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops of the input image making the image search more effective, and can iteratively adapt text search queries based on retrieved information, thereby enabling self-reflection and self-correction. Our approach relies on a two-stage training pipeline: a cold start supervised finetuning phase followed by an online reinforcement learning optimization. For training, we introduce DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated pipeline intermixed with real-world information from web search tools. This dataset contains diverse, multi-hop queries that integrate textual and visual information, teaching the model when to search, what to search for, which search tool to use and how to reason over the retrieved information. We conduct extensive experiments across a range of knowledge-intensive benchmarks to demonstrate the superiority of our approach. Finally, we analyze the results and provide insights that are valuable for advancing multimodal web-search.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/8MZCSRAM/Narayan et al. - 2025 - DeepMMSearch-R1 Empowering Multimodal LLMs in Multimodal Web Search.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/ZZTP4RIH/2510.html}
}

@misc{nikishinaCreatingTaxonomyRetrieval2025,
  title = {Creating a {{Taxonomy}} for {{Retrieval Augmented Generation Applications}}},
  author = {Nikishina, Irina and Sevgili, {\"O}zge and Li, Mahei Manhai and Biemann, Chris and Semmann, Martin},
  year = 2025,
  month = feb,
  number = {arXiv:2408.02854},
  eprint = {2408.02854},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.02854},
  urldate = {2025-09-13},
  abstract = {In this research, we develop a taxonomy to conceptualize a comprehensive overview of the constituting characteristics that define retrieval augmented generation (RAG) applications, facilitating the adoption of this technology for different application domains. To the best of our knowledge, no holistic RAG application taxonomies have been developed so far. We employ the method foreign to ACL and thus contribute to the set of methods in the taxonomy creation. It comprises four iterative phases designed to refine and enhance our understanding and presentation of RAG's core dimensions. We have developed a total of five meta-dimensions and sixteen dimensions to comprehensively capture the concept of RAG applications. Thus, the taxonomy can be used to better understand RAG applications and to derive design knowledge for future solutions in specific application domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/7EPEJFED/Nikishina et al. - 2025 - Creating a Taxonomy for Retrieval Augmented Generation Applications.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/2EMQUBVH/2408.html}
}

@misc{ocheSystematicReviewKey2025,
  title = {A {{Systematic Review}} of {{Key Retrieval-Augmented Generation}} ({{RAG}}) {{Systems}}: {{Progress}}, {{Gaps}}, and {{Future Directions}}},
  shorttitle = {A {{Systematic Review}} of {{Key Retrieval-Augmented Generation}} ({{RAG}}) {{Systems}}},
  author = {Oche, Agada Joseph and Folashade, Ademola Glory and Ghosal, Tirthankar and Biswas, Arpan},
  year = 2025,
  month = jul,
  number = {arXiv:2507.18910},
  eprint = {2507.18910},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.18910},
  urldate = {2025-09-13},
  abstract = {Retrieval-Augmented Generation (RAG) represents a major advancement in natural language processing (NLP), combining large language models (LLMs) with information retrieval systems to enhance factual grounding, accuracy, and contextual relevance. This paper presents a comprehensive systematic review of RAG, tracing its evolution from early developments in open domain question answering to recent state-of-the-art implementations across diverse applications. The review begins by outlining the motivations behind RAG, particularly its ability to mitigate hallucinations and outdated knowledge in parametric models. Core technical components-retrieval mechanisms, sequence-to-sequence generation models, and fusion strategies are examined in detail. A year-by-year analysis highlights key milestones and research trends, providing insight into RAG's rapid growth. The paper further explores the deployment of RAG in enterprise systems, addressing practical challenges related to retrieval of proprietary data, security, and scalability. A comparative evaluation of RAG implementations is conducted, benchmarking performance on retrieval accuracy, generation fluency, latency, and computational efficiency. Persistent challenges such as retrieval quality, privacy concerns, and integration overhead are critically assessed. Finally, the review highlights emerging solutions, including hybrid retrieval approaches, privacy-preserving techniques, optimized fusion strategies, and agentic RAG architectures. These innovations point toward a future of more reliable, efficient, and context-aware knowledge-intensive NLP systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/H82C5AT8/Oche et al. - 2025 - A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems Progress, Gaps, and Future.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/ZG6F75GU/2507.html}
}

@incollection{opasjumruskitAutomaticDataSheet2021,
  title = {Automatic {{Data Sheet Information Extraction}} for {{Supporting Model-Based Systems Engineering}}},
  booktitle = {Cooperative {{Design}}, {{Visualization}}, and {{Engineering}}},
  author = {Opasjumruskit, Kobkaew and Schindler, Sirko and Peters, Diana},
  editor = {Luo, Yuhua},
  year = 2021,
  volume = {12983},
  pages = {97--102},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-88207-5_10},
  urldate = {2025-11-01},
  abstract = {To describe modeling objects in Model-Based Systems Engineering (MBSE) tools, physical properties of these objects are often provided only in data sheets, which are not truly machine-readable. Previously, we proposed a product data hub to exchange spacecraft product information between manufacturers and various MBSE tools. However, issues with heterogeneous structures and semantics of information, such as differences in data format and vocabularies, persist. Using ontologies to maintain product descriptions can mitigate the heterogeneity problem by providing semantic descriptions and supporting different vocabularies for a single concept. To automatically and semantically obtain information from documents that contain tables, lists, and text, we developed an ontology-based information extraction tool. We present how to use the Data Sheets Annotation Tool (DSAT) for, either manually or automatically, extracting information from data sheets, and populating a database with the obtained data. Particularly, we emphasize on the usage of DSAT as a user interface for improving ontologies, which, in turn, are used for a (better) information extraction from the data sheets. Although DSAT is initially created for supporting collaborative systems engineering, it is not limited to the domain of spacecraft design. It can also be applied to other domains, where information needs to be extracted from a multitude of heterogeneous sources.},
  isbn = {978-3-030-88206-8 978-3-030-88207-5},
  langid = {english},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/XHUUN392/Opasjumruskit et al. - 2021 - Automatic Data Sheet Information Extraction for Supporting Model-Based Systems Engineering.pdf}
}

@misc{RAG,
  title = {{{RAG}}},
  urldate = {2025-11-02},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/en/model\_doc/rag},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/N6XICUYB/rag.html}
}

@misc{shengPdfTableUnifiedToolkit2024,
  title = {{{PdfTable}}: {{A Unified Toolkit}} for {{Deep Learning-Based Table Extraction}}},
  shorttitle = {{{PdfTable}}},
  author = {Sheng, Lei and Xu, Shuai-Shuai},
  year = 2024,
  month = sep,
  number = {arXiv:2409.05125},
  eprint = {2409.05125},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.05125},
  urldate = {2025-09-13},
  abstract = {Currently, a substantial volume of document data exists in an unstructured format, encompassing Portable Document Format (PDF) files and images. Extracting information from these documents presents formidable challenges due to diverse table styles, complex forms, and the inclusion of different languages. Several open-source toolkits, such as Camelot, Plumb a PDF (pdfnumber), and Paddle Paddle Structure V2 (PP-StructureV2), have been developed to facilitate table extraction from PDFs or images. However, each toolkit has its limitations. Camelot and pdfnumber can solely extract tables from digital PDFs and cannot handle image-based PDFs and pictures. On the other hand, PP-StructureV2 can comprehensively extract image-based PDFs and tables from pictures. Nevertheless, it lacks the ability to differentiate between diverse application scenarios, such as wired tables and wireless tables, digital PDFs, and image-based PDFs. To address these issues, we have introduced the PDF table extraction (PdfTable) toolkit. This toolkit integrates numerous open-source models, including seven table recognition models, four Optical character recognition (OCR) recognition tools, and three layout analysis models. By refining the PDF table extraction process, PdfTable achieves adaptability across various application scenarios. We substantiate the efficacy of the PdfTable toolkit through verification on a self-labeled wired table dataset and the open-source wireless Publicly Table Reconition Dataset (PubTabNet). The PdfTable code will available on Github: https://github.com/CycloneBoy/pdf\_table.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/V43J8SKA/Sheng und Xu - 2024 - PdfTable A Unified Toolkit for Deep Learning-Based Table Extraction.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/UDAPAVE7/2409.html}
}

@inproceedings{singhMIMOQAMultimodalInput2021,
  title = {{{MIMOQA}}: {{Multimodal Input Multimodal Output Question Answering}}},
  shorttitle = {{{MIMOQA}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Singh, Hrituraj and Nasery, Anshul and Mehta, Denil and Agarwal, Aishwarya and Lamba, Jatin and Srinivasan, Balaji Vasan},
  year = 2021,
  pages = {5317--5332},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.naacl-main.418},
  urldate = {2025-09-13},
  langid = {english},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/XEQYVFT2/Singh et al. - 2021 - MIMOQA Multimodal Input Multimodal Output Question Answering.pdf}
}

@inproceedings{smockPubTables1MComprehensiveTable2022,
  title = {{{PubTables-1M}}: {{Towards}} Comprehensive Table Extraction from Unstructured Documents},
  shorttitle = {{{PubTables-1M}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Smock, Brandon and Pesala, Rohith and Abraham, Robin},
  year = 2022,
  month = jun,
  pages = {4624--4632},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.00459},
  urldate = {2025-09-13},
  abstract = {Recently, significant progress has been made applying machine learning to the problem of table structure inference and extraction from unstructured documents. However, one of the greatest challenges remains the creation of datasets with complete, unambiguous ground truth at scale. To address this, we develop a new, more comprehensive dataset for table extraction, called PubTables-1M. PubTables-1M contains nearly one million tables from scientific articles, supports multiple input modalities, and contains detailed header and location information for table structures, making it useful for a wide variety of modeling approaches. It also addresses a significant source of ground truth inconsistency observed in prior datasets called oversegmentation, using a novel canonicalization procedure. We demonstrate that these improvements lead to a significant increase in training performance and a more reliable estimate of model performance at evaluation for table structure recognition. Further, we show that transformer-based object detection models trained on PubTables-1M produce excellent results for all three tasks of detection, structure recognition, and functional analysis without the need for any special customization for these tasks. Data and code will be released at https://github.com/microsoft/table-transformer.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-6946-3},
  langid = {english},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/FRNVI7PJ/Smock et al. - 2022 - PubTables-1M Towards comprehensive table extraction from unstructured documents.pdf}
}

@misc{somanObservationsBuildingRAG2024,
  title = {Observations on {{Building RAG Systems}} for {{Technical Documents}}},
  author = {Soman, Sumit and Roychowdhury, Sujoy},
  year = 2024,
  month = mar,
  number = {arXiv:2404.00657},
  eprint = {2404.00657},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.00657},
  urldate = {2025-10-15},
  abstract = {Retrieval augmented generation (RAG) for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting RAG and perform experiments to highlight best practices and potential challenges to build RAG systems for technical documents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/HMIRXQ28/Soman und Roychowdhury - 2024 - Observations on Building RAG Systems for Technical Documents.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/EUGW3BGG/2404.html}
}

@misc{tanakaVDocRAGRetrievalAugmentedGeneration2025,
  title = {{{VDocRAG}}: {{Retrieval-Augmented Generation}} over {{Visually-Rich Documents}}},
  shorttitle = {{{VDocRAG}}},
  author = {Tanaka, Ryota and Iki, Taichi and Hasegawa, Taku and Nishida, Kyosuke and Saito, Kuniko and Suzuki, Jun},
  year = 2025,
  month = apr,
  number = {arXiv:2504.09795},
  eprint = {2504.09795},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.09795},
  urldate = {2025-09-13},
  abstract = {We aim to develop a retrieval-augmented generation (RAG) framework that answers questions over a corpus of visually-rich documents presented in mixed modalities (e.g., charts, tables) and diverse formats (e.g., PDF, PPTX). In this paper, we introduce a new RAG framework, VDocRAG, which can directly understand varied documents and modalities in a unified image format to prevent missing information that occurs by parsing documents to obtain text. To improve the performance, we propose novel self-supervised pre-training tasks that adapt large vision-language models for retrieval by compressing visual information into dense token representations while aligning them with textual content in documents. Furthermore, we introduce OpenDocVQA, the first unified collection of open-domain document visual question answering datasets, encompassing diverse document types and formats. OpenDocVQA provides a comprehensive resource for training and evaluating retrieval and question answering models on visually-rich documents in an open-domain setting. Experiments show that VDocRAG substantially outperforms conventional text-based RAG and has strong generalization capability, highlighting the potential of an effective RAG paradigm for real-world documents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/NT84PLC8/Tanaka et al. - 2025 - VDocRAG Retrieval-Augmented Generation over Visually-Rich Documents.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/UA327HL4/2504.html}
}

@misc{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = 2023,
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2025-11-02},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/HC4QA78D/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/RGNY743X/1706.html}
}

@misc{wangViDoRAGVisualDocument2025,
  title = {{{ViDoRAG}}: {{Visual Document Retrieval-Augmented Generation}} via {{Dynamic Iterative Reasoning Agents}}},
  shorttitle = {{{ViDoRAG}}},
  author = {Wang, Qiuchen and Ding, Ruixue and Chen, Zehui and Wu, Weiqi and Wang, Shihang and Xie, Pengjun and Zhao, Feng},
  year = 2025,
  month = jun,
  number = {arXiv:2502.18017},
  eprint = {2502.18017},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.18017},
  urldate = {2025-09-13},
  abstract = {Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model's reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10\% on the competitive ViDoSeek benchmark. The code is available at https://github.com/Alibaba-NLP/ViDoRAG.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/J53EVXYN/Wang et al. - 2025 - ViDoRAG Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/WC9623KM/2502.html}
}

@misc{zhangLLMSpecificUtilityNew2025,
  title = {{{LLM-Specific Utility}}: {{A New Perspective}} for {{Retrieval-Augmented Generation}}},
  shorttitle = {{{LLM-Specific Utility}}},
  author = {Zhang, Hengran and Bi, Keping and Guo, Jiafeng and Zhang, Jiaming and Wang, Shuaiqiang and Yin, Dawei and Cheng, Xueqi},
  year = 2025,
  month = oct,
  number = {arXiv:2510.11358},
  eprint = {2510.11358},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.11358},
  urldate = {2025-10-15},
  abstract = {Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. While traditional retrieval focuses on relevance, RAG's effectiveness depends on the utility of retrieved passages, i.e., the usefulness in facilitating the generation of an accurate and comprehensive answer. Existing studies often treat utility as a generic attribute, ignoring the fact that different LLMs may benefit differently from the same passage due to variations in internal knowledge and comprehension ability. In this work, we introduce and systematically investigate the notion of LLM-specific utility. Through large-scale experiments across multiple datasets and LLMs, we demonstrate that human-annotated passages are not optimal for LLMs and that ground-truth utilitarian passages are not transferable across different LLMs. These findings highlight the necessity of adopting the LLM-specific utility in RAG research. Our findings indicate that some human-annotated passages are not ground-truth utilitarian passages for specific LLMs, partially due to the varying readability of queries and passages for LLMs, a tendency for which perplexity is a key metric. Based on these findings, we propose a benchmarking procedure for LLM-specific utility judgments. We evaluate existing utility judgment methods on six datasets and find that while verbalized methods using pseudo-answers perform robustly, LLMs struggle to assess utility effectively-failing to reject all passages for known queries and to select truly useful ones for unknown queries.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/HYRPERL3/Zhang et al. - 2025 - LLM-Specific Utility A New Perspective for Retrieval-Augmented Generation.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/XKBP9GX2/2510.html}
}

@misc{zhaoFRAGFederatedVector2024,
  title = {{{FRAG}}: {{Toward Federated Vector Database Management}} for {{Collaborative}} and {{Secure Retrieval-Augmented Generation}}},
  shorttitle = {{{FRAG}}},
  author = {Zhao, Dongfang},
  year = 2024,
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2410.13272},
  urldate = {2025-09-13},
  abstract = {This paper introduces \textbackslash textit\textbraceleft Federated Retrieval-Augmented Generation (FRAG)\textbraceright, a novel database management paradigm tailored for the growing needs of retrieval-augmented generation (RAG) systems, which are increasingly powered by large-language models (LLMs). FRAG enables mutually-distrusted parties to collaboratively perform Approximate \$k\$-Nearest Neighbor (ANN) searches on encrypted query vectors and encrypted data stored in distributed vector databases, all while ensuring that no party can gain any knowledge about the queries or data of others. Achieving this paradigm presents two key challenges: (i) ensuring strong security guarantees, such as Indistinguishability under Chosen-Plaintext Attack (IND-CPA), under practical assumptions (e.g., we avoid overly optimistic assumptions like non-collusion among parties); and (ii) maintaining performance overheads comparable to traditional, non-federated RAG systems. To address these challenges, FRAG employs a single-key homomorphic encryption protocol that simplifies key management across mutually-distrusted parties. Additionally, FRAG introduces a \textbackslash textit\textbraceleft multiplicative caching\textbraceright{} technique to efficiently encrypt floating-point numbers, significantly improving computational performance in large-scale federated environments. We provide a rigorous security proof using standard cryptographic reductions and demonstrate the practical scalability and efficiency of FRAG through extensive experiments on both benchmark and real-world datasets.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {Cryptography and Security (cs.CR),Databases (cs.DB),FOS: Computer and information sciences}
}

@misc{zhongMixofGranularityOptimizeChunking2025,
  title = {Mix-of-{{Granularity}}: {{Optimize}} the {{Chunking Granularity}} for {{Retrieval-Augmented Generation}}},
  shorttitle = {Mix-of-{{Granularity}}},
  author = {Zhong, Zijie and Liu, Hanwen and Cui, Xiaoya and Zhang, Xiaofan and Qin, Zengchang},
  year = 2025,
  month = jan,
  number = {arXiv:2406.00456},
  eprint = {2406.00456},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.00456},
  urldate = {2025-10-29},
  abstract = {Integrating information from various reference databases is a major challenge for Retrieval-Augmented Generation (RAG) systems because each knowledge source adopts a unique data structure and follows different conventions. Retrieving from multiple knowledge sources with one fixed strategy usually leads to under-exploitation of information. To mitigate this drawback, inspired by Mix-of-Expert, we introduce Mix-of-Granularity (MoG), a method that dynamically determines the optimal granularity of a knowledge source based on input queries using a router. The router is efficiently trained with a newly proposed loss function employing soft labels. We further extend MoG to MoG-Graph (MoGG), where reference documents are pre-processed as graphs, enabling the retrieval of distantly situated snippets. Experiments demonstrate that MoG and MoGG effectively predict optimal granularity levels, significantly enhancing the performance of the RAG system in downstream tasks. The code of both MoG and MoGG are released in https://github.com/ZGChung/Mix-of-Granularity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/GEY2CZGH/Zhong et al. - 2025 - Mix-of-Granularity Optimize the Chunking Granularity for Retrieval-Augmented Generation.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/ZSYJHV2T/2406.html}
}

@article{zhuLargeLanguageModels2025,
  title = {Large {{Language Models}} for {{Information Retrieval}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}} for {{Information Retrieval}}},
  author = {Zhu, Yutao and Yuan, Huaying and Wang, Shuting and Liu, Jiongnan and Liu, Wenhan and Deng, Chenlong and Chen, Haonan and Liu, Zheng and Dou, Zhicheng and Wen, Ji-Rong},
  year = 2025,
  month = sep,
  journal = {ACM Transactions on Information Systems},
  eprint = {2308.07107},
  primaryclass = {cs},
  pages = {3748304},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/3748304},
  urldate = {2025-11-01},
  abstract = {As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/niclas/snap/zotero-snap/common/Zotero/storage/U5NCKQB3/Zhu et al. - 2025 - Large Language Models for Information Retrieval A Survey.pdf;/home/niclas/snap/zotero-snap/common/Zotero/storage/E9894VPY/2308.html}
}
